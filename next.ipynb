{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import face_recognition\n",
    "from PIL import Image\n",
    "#from multiprocessing import Pool #mission impossible currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import torch.nn.functional as F\n",
    "from emotion_detection.fer_data_utils import SkResize, HistEq, AddChannel, ToRGB\n",
    "from vision_utils.custom_architectures import SepConvModel\n",
    "#extra added\n",
    "from vision_utils.custom_architectures import SepConvModel, initialize_model, PretrainedMT\n",
    "from multitask_rag.evaluate import predict_utk as test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHSEPCONV='/home/manish//Documents/democlassi/emotion-detection/sepconv_model_55_val_loss=1.175765.pth'\n",
    "PATHRESNETAGR='/home/manish/Documents/democlassi/other/resnet_model_21_val_loss=4.275671.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#emotion\n",
    "model = SepConvModel()\n",
    "model.load_state_dict(torch.load(PATHSEPCONV, map_location=\"cpu\"))\n",
    "\n",
    "#age-race-gender\n",
    "resnet_model_agr = PretrainedMT(model_name='resnet')\n",
    "resnet_model_agr.load_state_dict(torch.load(PATHRESNETAGR, map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    #transforms.Resize((500,500),Image.Resampling.LANCZOS),\n",
    "    transforms.Resize((200,200),Image.Resampling.LANCZOS),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these two function are needed doesn't work by just importing these function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def democlassiEvaluate(pil_image,):\n",
    "    \n",
    "    img_tensor= data_transforms(pil_image) \n",
    "    imge = pil_image.convert('L')\n",
    "    img = imge.resize((48,48),Image.Resampling.LANCZOS)\n",
    "    numpydata = asarray(img)\n",
    "    res1=predict_fer(numpydata,model,True)\n",
    "    # with resent model--age--race--gender\n",
    "    #res2=test_image(img_tensor,resnet_model)\n",
    "    #with sep_conv_model --age--gender---race\n",
    "    #res2=test_image(img_tensor,sep_conv_model)\n",
    "    #with VGG model ---age--race--gender\n",
    "    #res2=test_image(img_tensor,vgg_model_agr)\n",
    "    \n",
    "    #for resnet model\n",
    "    res2 = test_image(img_tensor,resnet_model_agr)\n",
    "    \n",
    "    #for sepconv_ model\n",
    "    #res2 = test_image(img_tensor,sepconv_model_agr)\n",
    "    return res1,res2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepfaceOutput(obj1):\n",
    "    print(\"Deepface comparision\")\n",
    "    print(\"----Deepface---\")\n",
    "    print(\"Age:---\",{obj1['age']},\"---\")\n",
    "    print(\"Gender:---\",{obj1['gender']},\"---\")\n",
    "    print(\"Race:---\",{obj1['dominant_race']},\"---\")\n",
    "    print(\"Emotion:---\",{obj1['dominant_emotion']},\"---\")\n",
    "    print('------------------------------------------- ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fer(image, model, transf_learn=True):\n",
    "\n",
    "    # process image\n",
    "    image = preprocess_fer(image, transf_learn)\n",
    "    \n",
    "    # prepare model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "\n",
    "    # predict probabilities\n",
    "    emotion = F.softmax(model(image), dim=1).detach().to('cpu').numpy()[0]\n",
    "    target_names = ['Angry', 'Disgusted', 'Afraid', 'Happy', 'Sad', 'Surprised', 'Neutral']\n",
    "    pred_label = target_names[np.argmax(emotion)]\n",
    "\n",
    "    emotion_probs = dict(zip(target_names, emotion))\n",
    "\n",
    "    return emotion_probs, pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fer(image, transf_learn):\n",
    "    if transf_learn:\n",
    "        transf = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    else:\n",
    "        transf = transforms.Compose([\n",
    "            HistEq(),\n",
    "            AddChannel(),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        print(transf(image))  \n",
    "\n",
    "    return transf(image).to(torch.float32).unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDemoClassiOutput(res1,res2):\n",
    "    print(\"Democlassi comparision\")\n",
    "    print(\"---Democlassi---\")\n",
    "    print(\"Age:---\",{round(res2[0], 0)},\"---\")\n",
    "    print(\"Gender:---\",{res2[2]},\"---\")\n",
    "    print(\"Race:---\",{res2[4]},\"---\")\n",
    "    print(\"Emotion:---\",{res1[1]},\"---\") #res1 for emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manish/fotos-form-download/image327.jpg\n",
      "num of Face_locations found: 1\n",
      "Face location has been found\n",
      "Deep-face evaluation: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: age:  25%|██▌       | 1/4 [00:00<00:00,  5.02it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  50%|█████     | 2/4 [00:01<00:01,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 994ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race:  75%|███████▌  | 3/4 [00:02<00:01,  1.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deepface comparision\n",
      "----Deepface---\n",
      "Age:--- {39} ---\n",
      "Gender:--- {'Man'} ---\n",
      "Race:--- {'white'} ---\n",
      "Emotion:--- {'neutral'} ---\n",
      "------------------------------------------- \n",
      "Deepface analysis compelete\n",
      "******************************************************************\n",
      "\n",
      "democlassi-evaluation:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {30.0} ---\n",
      "Gender:--- {'Man'} ---\n",
      "Race:--- {'White'} ---\n",
      "Emotion:--- {'Sad'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/fotos-form-download/image264.jpg\n",
      "num of Face_locations found: 9\n",
      "Face location has been found\n",
      "Deep-face evaluation: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 81ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: age:  25%|██▌       | 1/4 [00:00<00:00,  5.23it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  50%|█████     | 2/4 [00:01<00:02,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 695ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race:  75%|███████▌  | 3/4 [00:02<00:00,  1.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 662ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deepface comparision\n",
      "----Deepface---\n",
      "Age:--- {28} ---\n",
      "Gender:--- {'Man'} ---\n",
      "Race:--- {'white'} ---\n",
      "Emotion:--- {'happy'} ---\n",
      "------------------------------------------- \n",
      "Deepface analysis compelete\n",
      "******************************************************************\n",
      "\n",
      "democlassi-evaluation:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {30.0} ---\n",
      "Gender:--- {'Man'} ---\n",
      "Race:--- {'White'} ---\n",
      "Emotion:--- {'Happy'} ---\n",
      "------------------------------------------- \n",
      "Face location has been found\n",
      "Deep-face evaluation: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: age:  25%|██▌       | 1/4 [00:00<00:00,  6.73it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 732ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  50%|█████     | 2/4 [00:01<00:01,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 895ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race:  75%|███████▌  | 3/4 [00:02<00:00,  1.25it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 900ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deepface comparision\n",
      "----Deepface---\n",
      "Age:--- {31} ---\n",
      "Gender:--- {'Man'} ---\n",
      "Race:--- {'white'} ---\n",
      "Emotion:--- {'happy'} ---\n",
      "------------------------------------------- \n",
      "Deepface analysis compelete\n",
      "******************************************************************\n",
      "\n",
      "democlassi-evaluation:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {29.0} ---\n",
      "Gender:--- {'Man'} ---\n",
      "Race:--- {'White'} ---\n",
      "Emotion:--- {'Happy'} ---\n",
      "------------------------------------------- \n",
      "Face location has been found\n",
      "Deep-face evaluation: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: age:  25%|██▌       | 1/4 [00:00<00:00,  8.11it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 735ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: gender:  50%|█████     | 2/4 [00:00<00:01,  1.80it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "import gc \n",
    "\n",
    "\n",
    "folder_dir = \"/home/manish/fotos-form-download\"\n",
    "save_dir= \"/home/manish/Documents\"\n",
    "for images in os.listdir(folder_dir):\n",
    "    gc.collect()\n",
    "    if (images.endswith(\".png\") or images.endswith(\".jpg\")or images.endswith(\".jpeg\")):\n",
    "            new=folder_dir+\"/\"+images\n",
    "            print(new)\n",
    "            image = face_recognition.load_image_file(new)\n",
    "            face_locations = face_recognition.face_locations(image, number_of_times_to_upsample=0, model=\"resnet\")\n",
    "            print(\"num of Face_locations found: \" + str(len(face_locations)))\n",
    "            \n",
    "            if len(face_locations) != 0 : #check if the image contains people\n",
    "                for face_location in face_locations:\n",
    "                    \n",
    "                    print(\"Face location has been found\")\n",
    "                    top, right, bottom, left = face_location\n",
    "                    face_image = image[top:bottom,left:right]\n",
    "                    pil_img = Image.fromarray(face_image)\n",
    "                    pil_image = pil_img.resize((200,200),Image.Resampling.LANCZOS)\n",
    "\n",
    "                    fileLocation=save_dir+'/'+'tmp.jpg'\n",
    "                    pil_image.save(fileLocation,\"JPEG\")\n",
    "                    \n",
    "                    print(\"Deep-face evaluation: \\n\")\n",
    "                    obj1= DeepFace.analyze(img_path=fileLocation, enforce_detection=False)\n",
    "                    deepfaceOutput(obj1)\n",
    "                    print(\"Deepface analysis compelete\")\n",
    "                    print(\"******************************************************************\\n\")\n",
    "                    print(\"democlassi-evaluation:\\n\")\n",
    "                    res1, res2 = democlassiEvaluate(pil_image)\n",
    "                    printDemoClassiOutput(res1,res2)\n",
    "                    print('------------------------------------------- ')\n",
    "            else:\n",
    "                print(\"Face lOcation not found\")\n",
    "                \n",
    "                gc.collect()\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if face_recognition can locate all the image position "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
