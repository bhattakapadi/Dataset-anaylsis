{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from emotion_detection.fer_data_utils import SkResize, HistEq, AddChannel, ToRGB\n",
    "from vision_utils.custom_architectures import SepConvModel\n",
    "#extra added\n",
    "from vision_utils.custom_architectures import SepConvModelMT, SepConvModel, initialize_model, PretrainedMT\n",
    "from emotion_detection.evaluate import predict_fer as eval_man\n",
    "from vision_utils.custom_architectures import PretrainedMT\n",
    "from multitask_rag.evaluate import predict_utk as test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHRESNETAGR='/home/manish/Documents/democlassi/other/resnet_model_21_val_loss=4.275671.pth'\n",
    "PATHSEPCONVAGR='/home/manish/Documents/democlassi/other/sep_conv_adam_model_33_val_loss=4.714899.pth'\n",
    "PATHVGGAGR='/home/manish/Documents/democlassi/other/vgg_model_21_val_loss=4.139335.pth'\n",
    "\n",
    "PATHIMAGE='/home/manish/Documents/fotos/pic2.jpg'\n",
    "#PATHIMAGE='/home/manish/Documents/fotos/two-people.jpg'\n",
    "PATHSEPCONV='/home/manish//Documents/democlassi/emotion-detection/sepconv_model_55_val_loss=1.175765.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#emotion\n",
    "model = SepConvModel()\n",
    "model.load_state_dict(torch.load(PATHSEPCONV, map_location=\"cpu\"))\n",
    "\n",
    "#age-race-gender\n",
    "resnet_model_agr = PretrainedMT(model_name='resnet')\n",
    "resnet_model_agr.load_state_dict(torch.load(PATHRESNETAGR, map_location=\"cpu\"))\n",
    "\n",
    "\n",
    "#age-race-gender\n",
    "sep_conv_model_agr = SepConvModelMT()\n",
    "sep_conv_model_agr.load_state_dict(torch.load(PATHSEPCONVAGR, map_location=\"cpu\")) \n",
    "\n",
    "#age-race-gender\n",
    "vgg_model_agr = PretrainedMT(model_name='vgg')\n",
    "vgg_model_agr.load_state_dict(torch.load(PATHVGGAGR, map_location=\"cpu\")) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fer(image, transf_learn):\n",
    "    if transf_learn:\n",
    "        transf = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    else:\n",
    "        transf = transforms.Compose([\n",
    "            HistEq(),\n",
    "            AddChannel(),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        print(transf(image))  \n",
    "\n",
    "    return transf(image).to(torch.float32).unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fer(image, model, transf_learn=True):\n",
    "\n",
    "    # process image\n",
    "    image = preprocess_fer(image, transf_learn)\n",
    "    \n",
    "    # prepare model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "\n",
    "    # predict probabilities\n",
    "    emotion = F.softmax(model(image), dim=1).detach().to('cpu').numpy()[0]\n",
    "    target_names = ['Angry', 'Disgusted', 'Afraid', 'Happy', 'Sad', 'Surprised', 'Neutral']\n",
    "    pred_label = target_names[np.argmax(emotion)]\n",
    "\n",
    "    emotion_probs = dict(zip(target_names, emotion))\n",
    "\n",
    "    return emotion_probs, pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def democlassiEvaluate(pil_image,):\n",
    "    \n",
    "    img_tensor= data_transforms(pil_image) \n",
    "    imge = pil_image.convert('L')\n",
    "    img = imge.resize((48,48),Image.Resampling.LANCZOS)\n",
    "    numpydata = asarray(img)\n",
    "    res1=predict_fer(numpydata,model,True)\n",
    "    # with resent model--age--race--gender\n",
    "    #res2=test_image(img_tensor,resnet_model)\n",
    "    #with sep_conv_model --age--gender---race\n",
    "    #res2=test_image(img_tensor,sep_conv_model)\n",
    "    #with VGG model ---age--race--gender\n",
    "    res2=test_image(img_tensor,vgg_model_agr)\n",
    "    \n",
    "    #for resnet model\n",
    "    res2 = test_image(img_tensor,resnet_model_agr)\n",
    "    \n",
    "    #for sepconv_ model\n",
    "    res2 = test_image(img_tensor,sepconv_model_agr)\n",
    "    return res1,res2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDemoClassiOutput(res1,res2):\n",
    "    print(\"Democlassi comparision\")\n",
    "    print(\"---Democlassi---\")\n",
    "    print(\"Age:---\",{round(res2[0], 0)},\"---\")\n",
    "    print(\"Gender:---\",{res2[2]},\"---\")\n",
    "    print(\"Race:---\",{res2[4]},\"---\")\n",
    "    print(\"Emotion:---\",{res1[1]},\"---\") #res1 for emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manish/Documents/fotos/pic3.jpg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {24.0} ---\n",
      "Gender:--- {'Man'} ---\n",
      "Race:--- {'Black'} ---\n",
      "Emotion:--- {'Neutral'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/pic8.jpg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {29.0} ---\n",
      "Gender:--- {'Woman'} ---\n",
      "Race:--- {'White'} ---\n",
      "Emotion:--- {'Happy'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/pic0.jpg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {23.0} ---\n",
      "Gender:--- {'Man'} ---\n",
      "Race:--- {'White'} ---\n",
      "Emotion:--- {'Happy'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/pic12.jpeg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {28.0} ---\n",
      "Gender:--- {'Woman'} ---\n",
      "Race:--- {'Indian'} ---\n",
      "Emotion:--- {'Neutral'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/pic1.jpg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {23.0} ---\n",
      "Gender:--- {'Man'} ---\n",
      "Race:--- {'Black'} ---\n",
      "Emotion:--- {'Angry'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/two-people.jpg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {24.0} ---\n",
      "Gender:--- {'Man'} ---\n",
      "Race:--- {'White'} ---\n",
      "Emotion:--- {'Happy'} ---\n",
      "------------------------------------------- \n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {25.0} ---\n",
      "Gender:--- {'Woman'} ---\n",
      "Race:--- {'Black'} ---\n",
      "Emotion:--- {'Happy'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/pic13.jpg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {23.0} ---\n",
      "Gender:--- {'Woman'} ---\n",
      "Race:--- {'Black'} ---\n",
      "Emotion:--- {'Angry'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/pic14.jpg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {23.0} ---\n",
      "Gender:--- {'Man'} ---\n",
      "Race:--- {'White'} ---\n",
      "Emotion:--- {'Surprised'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/pic2.jpg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {25.0} ---\n",
      "Gender:--- {'Man'} ---\n",
      "Race:--- {'White'} ---\n",
      "Emotion:--- {'Angry'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/pic10.jpg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {28.0} ---\n",
      "Gender:--- {'Woman'} ---\n",
      "Race:--- {'White'} ---\n",
      "Emotion:--- {'Happy'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/pic11.jpeg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {25.0} ---\n",
      "Gender:--- {'Woman'} ---\n",
      "Race:--- {'White'} ---\n",
      "Emotion:--- {'Afraid'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/pic4.jpg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {21.0} ---\n",
      "Gender:--- {'Man'} ---\n",
      "Race:--- {'White'} ---\n",
      "Emotion:--- {'Neutral'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/pic6.jpg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {28.0} ---\n",
      "Gender:--- {'Woman'} ---\n",
      "Race:--- {'Black'} ---\n",
      "Emotion:--- {'Happy'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/pic5.jpg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {29.0} ---\n",
      "Gender:--- {'Woman'} ---\n",
      "Race:--- {'White'} ---\n",
      "Emotion:--- {'Neutral'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/pic7.jpg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {26.0} ---\n",
      "Gender:--- {'Man'} ---\n",
      "Race:--- {'White'} ---\n",
      "Emotion:--- {'Neutral'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "/home/manish/Documents/fotos/pic9.jpg\n",
      "Democlassi comparision\n",
      "---Democlassi---\n",
      "Age:--- {21.0} ---\n",
      "Gender:--- {'Woman'} ---\n",
      "Race:--- {'Indian'} ---\n",
      "Emotion:--- {'Surprised'} ---\n",
      "------------------------------------------- \n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "({'Angry': 0.038871508, 'Disgusted': 0.007337459, 'Afraid': 0.22377424, 'Happy': 0.08318108, 'Sad': 0.041756347, 'Surprised': 0.55440956, 'Neutral': 0.050669767}, 'Surprised')\n",
      "(20.83922, {'Man': 0.3978376, 'Woman': 0.60216236}, 'Woman', {'White': 0.14798114, 'Black': 0.107716724, 'Asian': 0.051423166, 'Indian': 0.5610967, 'Unknown-race': 0.1317822}, 'Indian')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "\n",
    "folder_dir = \"/home/manish/Documents/fotos\"\n",
    "save_dir = \"/home/manish/Documents/fotos/after-cropping\"\n",
    "\n",
    "for images in os.listdir(folder_dir):\n",
    "    if (images.endswith(\".png\") or images.endswith(\".jpg\")or images.endswith(\".jpeg\")):\n",
    "            new=folder_dir+\"/\"+images\n",
    "            print(new)\n",
    "            new_dir=save_dir+\"/\"+\"presave.jpg\"\n",
    "            image = Image.open(new)\n",
    "\n",
    "            tmp= image.size\n",
    "            if( tmp[0]*tmp[1] > 400000 ):  #or tmp[0]*tmp[1] < 50625):\n",
    "                image =image.resize((600,600),Image.Resampling.LANCZOS)\n",
    "                \n",
    "            if( tmp[0]*tmp[1] < 50625):  #or tmp[0]*tmp[1] < 50625):\n",
    "                image =image.resize((225,225),Image.Resampling.LANCZOS)\n",
    "                \n",
    "            #image = image.convert(\"RGB\")\n",
    "            image.save(new_dir,\"JPEG\")\n",
    "            \n",
    "            image = face_recognition.load_image_file(new)\n",
    "            face_locations = face_recognition.face_locations(image, number_of_times_to_upsample=0, model=\"resnet\")\n",
    "           \n",
    "            for face_location in face_locations:\n",
    "                # Print the location of each face in this image\n",
    "                top, right, bottom, left = face_location\n",
    "                face_image = image[top:bottom, left:right]\n",
    "                pil_image = Image.fromarray(face_image)\n",
    "                \n",
    "                #democlassi\n",
    "                res1, res2 = democlassiEvaluate(pil_image)\n",
    "                printDemoClassiOutput(res1,res2)\n",
    "                print( '------------------------------------------- ')\n",
    "    print('++++++++++++++++++++++++++++++++++++++++++')\n",
    "print(res1)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (2236667598.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_9808/2236667598.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    if(images.endswith(\".png\") or images.endswith(\".jpg\")or images.endswith(\".jpeg\")):\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "folder_dir=\"/home/manish/Documents/fotos\"\n",
    "for images in os.listdir(folder_dir):\n",
    "    if(images.endswith(\".png\") or images.endswith(\".jpg\")or images.endswith(\".jpeg\")):\n",
    "        print(images)\n",
    "        image1 = Image.open(PATHIMAGE)\n",
    "        img_tensor= data_transforms(image1)\n",
    "        imge = Image.open(PATHIMAGE).convert('L')\n",
    "        img = imge.resize((48,48),Image.Resampling.LANCZOS)\n",
    "        numpydata = asarray(img)\n",
    "        res1,res2= democlassResult(img_tensor,numpydata)\n",
    "        printDemoClassiOutput(res1,res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1=predict_fer(numpydata,model,True) #democlassi  predict_fer ---> emotion\n",
    "res2=test_image(img_tensor,resnet_model) #democlassi  test_image ---> age,race,gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output comparision\n",
      "---Democlassi--- vs---Deepface---\n",
      "Age:--- {29.0} ---vs--- {22} ---\n",
      "Gender:--- {'Woman'} ---vs--- {'Man'} ---\n",
      "Race:--- {'Asian'} ---vs--- {'latino hispanic'} ---\n",
      "Emotion:--- {'Neutral'} ---vs--- {'fear'} ---\n"
     ]
    }
   ],
   "source": [
    "print(\"Output comparision\")\n",
    "print(\"---Democlassi--- vs---Deepface---\")\n",
    "print(\"Age:---\",{round(res2[0], 0)},\"---vs---\",{obj1['age']},\"---\")\n",
    "print(\"Gender:---\",{res2[2]},\"---vs---\",{obj1['gender']},\"---\")\n",
    "print(\"Race:---\",{res2[4]},\"---vs---\",{obj1['dominant_race']},\"---\")\n",
    "print(\"Emotion:---\",{res1[1]},\"---vs---\",{obj1['dominant_emotion']},\"---\")#res1 for emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'Angry': 0.040792275, 'Disgusted': 0.0008434682, 'Afraid': 0.027870916, 'Happy': 0.20536155, 'Sad': 0.08617818, 'Surprised': 0.0066849818, 'Neutral': 0.6322686}, 'Neutral')\n"
     ]
    }
   ],
   "source": [
    "print(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29.182735, {'Man': 0.13909152, 'Woman': 0.8609085}, 'Woman', {'White': 0.15336668, 'Black': 0.22007346, 'Asian': 0.5476374, 'Indian': 0.003787156, 'Unknown-race': 0.07513531}, 'Asian')\n"
     ]
    }
   ],
   "source": [
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
